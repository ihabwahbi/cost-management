Best Practices
Best Practices for Search
Learn how to optimize your queries, refine search filters, and leverage advanced parameters for better performance.

‚Äã
Optimizing your query
‚Äã
1. Keep your query under 400 characters
For efficient processing, keep your query concise‚Äîunder 400 characters. Think of it as a query for an agent performing web search, not long-form prompts. If your query exceeds the limit, you‚Äôll see this error:

Copy

Ask AI
{
  "detail": {
    "error": "Query is too long. Max query length is 400 characters."
  }
}
‚Äã
2. Break your query into smaller sub-queries
If your query is complex or covers multiple topics, consider breaking it into smaller, more focused sub-queries and sending them as separate requests.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Breaking the query into smaller, more focused sub-queries.
{
   "query":"Competitors of company ABC."
}
{
   "query":"Financial performance of company ABC."
}
{
   "query":"Recent developments of company ABC."
}
{
   "query":"Latest industry trends related to ABC."
}
‚Äã
Optimizing your request parameters
‚Äã
max_results  (Limiting the number of results)
Limits the number of search results (default is 5).

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Customizing max_results based on your needs, limiting the results to 10 to improve relevance and focus on the most relevant sources.
{
  "query": "Info about renewable energy technologies",
  "max_results": 10
}
‚Äã
content (NLP-based snippet)
Provides a summarized content snippet.
Helps in quickly understanding the main context without extracting full content.
When search_depth is set to advanced , it extracts content closely aligned with your query, surfacing the most valuable sections of a web page rather than a generic summary. Additionally, it uses chunks_per_source to determine the number of content chunks to return per source.
‚Äã
search_depth=advanced  (Ideal for higher relevance in search results)
Retrieves the most relevant content snippets for your query.
By setting include_raw_content to true, you can increase the likelihood of enhancing retrieval precision and retrieving the desired number of chunks_per_source.

‚úÖ Good

üëç Good

‚ùå Bad

Copy

Ask AI
// Using search_depth=advanced and chunks_per_source for a query to get the most relevant content, and enabling include_raw_content.
{
  "query": "How many countries use Monday.com?",
  "search_depth": "advanced",
  "chunks_per_source": 3,
  "include_raw_content": true
}
‚Äã
time_range (Filtering by Date)
Restricts search results to a specific time frame.

‚úÖ Good

üëç Moderate

Copy

Ask AI
// Using time_range to filter sources from the past month.
{
  "query": "latest trends in machine learning",
  "time_range": "month"
}
‚Äã
start_date and  end_date (Filtering by Specific Date Range)
Filters search results published within a specified date range.

‚úÖ Good

üëç Moderate

Copy

Ask AI
// Using start_date and end_date to filter results published between specific dates.
{ 
    "query": "latest trends in machine learning",
    "start_date": "2025-01-01",
    "end_date": "2025-02-01" 
} 
‚Äã
include_raw_content (Extracted web content)
Set to true to return the full extracted content of the web page, useful for deeper content analysis. However, the most recommended approach for extracting web page content is using a two-step process:
Search: Retrieve relevant URLs.
Extract: Extract content from those URLs.
For more information on this two-step process, please refer to the Best Practices for the Extract API.

‚úÖ Good

üëç Moderate

Copy

Ask AI
// Using include_raw_content to retrieve full content for comprehensive analysis.
{
    "query": "The impact of AI in healthcare",
    "include_raw_content": true
}
‚Äã
topic=news (Filtering news sources)
Limits results to news-related sources.
Includes published_date metadata.
days specifies the number of days back from the current date to include in the results. The default is 3.
Useful for getting real-time updates, particularly about politics, sports, and major current events covered by mainstream media sources.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Using "topic=news" and "days=1" to get the latest updates from news sources.
{
  "query": "What happened today in NY?",
  "topic": "news",
  "days": 1
}
‚Äã
auto_parameters (Automatically Optimizing Search Parameters)
When enabled, Tavily intelligently adjusts search parameters based on the query‚Äôs intent.
Explicitly set values always override the automatic ones.
Note: search_depth may default to advanced, using 2 API credits per request. To control cost, set it manually to basic.

‚úÖ Good

üëç Moderate

Copy

Ask AI
// auto_parameters enabled with manual override to control cost and output. 
{ 
    "query": "impact of AI in education policy", 
    "auto_parameters": true, 
    "search_depth": "basic", // Overrides 'advanced' 
    "include_answer": true, 
    "max_results": 10 
} 
‚Äã
include_domains (Restricting searches to specific domains)
Limits searches to predefined trusted domains.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Using include_domains to restrict search for more domain-specific information.
{
  "query": "What is the professional background of the CEO at Google?",
  "include_domains": ["linkedin.com/in"]
}
Minimize the number of domains in the include_domains list and make sure they are relevant to your search query.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Using a concise list of 3 relevant domains to refine search results effectively.
{
    "query": "What are the latest funding rounds for AI startups?",
    "include_domains": [ "crunchbase.com", "techcrunch.com", "pitchbook.com" ]
 }
‚Äã
exclude_domains (Excluding specific domains)
Filters out results from specific domains.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Excluding unrelated domains to US economy trends, ensuring that irrelevant sources are filtered out.
{
   "query": "US economy trends in 2025",
   "exclude_domains": ["espn.com","vogue.com"]
}
Minimize the number of domains in the exclude_domains list to ensure you only exclude domains that are truly irrelevant to your query.

‚úÖ Good

‚ùå Bad

Copy

Ask AI
// Using a concise list of 3 domains to exclude from the search results.
{
    "query": "US fashion trends in 2025",
    "exclude_domains": ["nytimes.com","forbes.com","bloomberg.com"]
}
‚Äã
Controlling search results by website region
Example: Limit to U.S.-based websites (.com domain):

Copy

Ask AI
{
    "query": "latest AI research",
    "include_domains": ["*.com"]
}
Example: Exclude Icelandic websites (.is domain):

Copy

Ask AI
{
    "query": "global economic trends",
    "exclude_domains": ["*.is"]
}
Example: Boost results from a specific country using the country parameter:

Copy

Ask AI
{
    "query": "tech startup funding",
    "topic": "general",
    "country": "united states"
}
‚Äã
Combining include and exclude domains
Restrict search to .com but exclude example.com:

Copy

Ask AI
{
    "query": "AI industry news",
    "include_domains": ["*.com"],
    "exclude_domains": ["example.com"]
}
‚Äã
Asynchronous API calls with Tavily
Use async/await to ensure non-blocking API requests.
Initialize AsyncTavilyClient once and reuse it for multiple requests.
Use asyncio.gather for handling multiple queries concurrently.
Implement error handling to manage API failures gracefully.
Limit concurrent requests to avoid hitting rate limits.
Example:

Copy

Ask AI
import asyncio
from tavily import AsyncTavilyClient

# Initialize Tavily client
tavily_client = AsyncTavilyClient("tvly-YOUR_API_KEY")

async def fetch_and_gather():
    queries = ["latest AI trends", "future of quantum computing"]

    # Perform search and continue even if one query fails (using return_exceptions=True)
    try:
        responses = await asyncio.gather(*(tavily_client.search(q) for q in queries), return_exceptions=True)

        # Handle responses and print
        for response in responses:
            if isinstance(response, Exception):
                print(f"Search query failed: {response}")
            else:
                print(response)

    except Exception as e:
        print(f"Error during search queries: {e}")

# Run the function
asyncio.run(fetch_and_gather())
‚Äã
Optimizing search results with post-processing techniques
When working with Tavily‚Äôs Search API, refining search results through post-processing techniques can significantly enhance the relevance of the retrieved information.
‚Äã
Combining LLMs with Keyword Filtering
One of the most effective ways to refine search results is by using a combination of LLMs and deterministic keyword filtering.
LLMs can analyze search results in a more contextual and semantic manner, understanding the deeper meaning of the text.
Keyword filtering offers a rule-based approach to eliminate irrelevant results based on predefined terms, ensuring a balance between flexibility and precision.
‚Äã
How it works
By applying keyword filters before or after processing results with an LLM, you can:
Remove results that contain specific unwanted terms.
Prioritize articles that contain high-value keywords relevant to your use case.
Improve efficiency by reducing the number of search results requiring further LLM processing.
‚Äã
Utilizing metadata for improved post-processing
Tavily‚Äôs Search API provides rich metadata that can be leveraged to refine and prioritize search results. By incorporating metadata into post-processing logic, you can improve precision in selecting the most relevant content.
‚Äã
Key metadata fields and their Functions
title: Helps in identifying articles that are more likely to be relevant based on their headlines. Filtering results by keyword occurrences in the title can improve result relevancy.
raw_content: Provides the extracted content from the web page, allowing deeper analysis. If the content does not provide enough information, raw content can be useful for further filtering and ranking. You can also use the Extract API with a two-step extraction process. For more information, see Best Practices for Extract API.
score: Represents the relevancy between the query and the retrieved content snippet. Higher scores typically indicate better matches.
content: Offers a general summary of the webpage, providing a quick way to gauge relevance without processing the full content. When search_depth is set to advanced, the content is more closely aligned with the query, offering valuable insights.
‚Äã
Enhancing post-processing with metadata
By leveraging these metadata elements, you can:
Sort results based on scores, prioritizing high-confidence matches.
Perform additional filtering based on title or content to refine search results.
‚Äã
Understanding the score Parameter
Tavily assigns a score to each search result, indicating how well the content aligns with the query. This score helps in ranking and selecting the most relevant results.
‚Äã
What does the score mean?
The score is a numerical measure of relevance between the content and the query.
A higher score generally indicates that the result is more relevant to the query.
There is no fixed threshold that determines whether a result is useful. The ideal score cutoff depends on the specific use case.
‚Äã
Best practices for using scores
Set a minimum score threshold to exclude low-relevance results automatically.
Analyze the distribution of scores within a search response to adjust thresholds dynamically.
Combine similarity scores with other metadata fields (e.g., url, content) to improve ranking strategies.
‚Äã
Using regex-based data extraction
In addition to leveraging LLMs and metadata for refining search results, Python‚Äôs re.search and re.findall methods can play a crucial role in post-processing by allowing you to parse and extract specific data from the raw_content. These methods enable pattern-based filtering and extraction, enhancing the precision and relevance of the processed results.
‚Äã
Benefits of using re.search and re.findall
Pattern Matching: Both methods are designed to search for specific patterns in text, which is ideal for structured data extraction.
Efficiency: These methods help automate the extraction of specific elements from large datasets, improving post-processing efficiency.
Flexibility: You can define custom patterns to match a variety of data types, from dates and addresses to keywords and job titles.
‚Äã
How they work
re.search: Scans the content for the first occurrence of a specified pattern and returns a match object, which can be used to extract specific parts of the text.
Example:

Copy

Ask AI
import re
text = "Company: Tavily, Location: New York"
match = re.search(r"Location: (\w+)", text)
if match:
    print(match.group(1))  # Output: New York
re.findall: Returns a list of all non-overlapping matches of a pattern in the content, making it suitable for extracting multiple instances of a pattern.
Example:

Copy

Ask AI
text = "Contact: john@example.com, support@tavily.com"
emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text)
print(emails)  # Output: ['john@example.com', 'support@tavily.com']
‚Äã
Common use cases for post-processing
Content Filtering: Use re.search to identify sections or specific patterns in content (e.g., dates, locations, company names).
Data Extraction: Use re.findall to extract multiple instances of specific data points (e.g., phone numbers, emails).
Improving Relevance: Apply regex patterns to remove irrelevant content, ensuring that only the most pertinent information remains.
By leveraging post-processing techniques such as LLM-assisted filtering, metadata analysis, and score-based ranking, along with regex-based data extraction, you can optimize Tavily‚Äôs Search API results for better relevance. Incorporating these methods into your workflow will help you extract high-quality insights tailored to your needs.

Best Practices for Extract
Learn the best practices for web content extraction process

‚Äã
Extracting web content using Tavily
Efficiently extracting content from web pages is crucial for AI-powered applications. Tavily provides two main approaches to content extraction, each suited for different use cases.
‚Äã
1. One-step extraction: directly retrieve raw_content
You can extract web content by enabling include_raw_content = true when making a Tavily Search API call. This allows you to retrieve both search results and extracted content in a single step.
However, this can increase latency because you may extract raw content from sources that are not relevant in the first place. It‚Äôs recommended to split the process into two steps: running multiple sub-queries to expand the pool of sources, then curating the most relevant documents based on content snippets or source scores. By extracting raw content from the most relevant sources, you get high-quality RAG documents.
‚Äã
2. Two-step process: search, then extract
For better accuracy and customization, we recommend a two-step process:
‚Äã
Step 1: Search
Use the Tavily Search API to retrieve relevant web pages, which output URLs.
‚Äã
Step 2: Extract
Use the Tavily Extract API to fetch the full content from the most relevant URLs.
Example:

Copy

Ask AI
import asyncio
from tavily import AsyncTavilyClient

tavily_client = AsyncTavilyClient(api_key="tvly-YOUR_API_KEY")

async def fetch_and_extract():
   # Define the queries with search_depth and max_results inside the query dictionary
   queries = [
       {"query": "AI applications in healthcare", "search_depth": "advanced", "max_results": 10},
       {"query": "ethical implications of AI in healthcare", "search_depth": "advanced", "max_results": 10},
       {"query": "latest trends in machine learning healthcare applications", "search_depth": "advanced",
        "max_results": 10},
       {"query": "AI and healthcare regulatory challenges", "search_depth": "advanced", "max_results": 10}
   ]

   # Perform the search queries concurrently, passing the entire query dictionary
   responses = await asyncio.gather(*[tavily_client.search(**q) for q in queries])

   # Filter URLs with a score greater than 0.5. Alternatively, you can use a re-ranking model or an LLM to identify the most relevant sources, or cluster your documents and extract content only from the most relevant cluster
   relevant_urls = []
   for response in responses:
       for result in response.get('results', []):
           if result.get('score', 0) > 0.5:
               relevant_urls.append(result.get('url'))

   # Extract content from the relevant URLs
   extracted_data = await asyncio.gather(*(tavily_client.extract(url) for url in relevant_urls))

   # Print the extracted content
   for data in extracted_data:
       print(data)

# Run the function
asyncio.run(fetch_and_extract())
‚Äã
Pros of two-Step extraction
‚úÖ More control ‚Äì Extract only from selected URLs.
‚úÖ Higher accuracy ‚Äì Filter out irrelevant results before extraction.
‚úÖ Advanced extraction capabilities ‚Äì Using search_depth = "advanced".
‚Äã
Cons of two-step extraction
‚ùå slightly more expensive.
‚Äã
Using advanced extraction
Using extract_depth = "advanced" in the Extract API allows for more comprehensive content retrieval. This mode is particularly useful when dealing with:
Complex web pages with dynamic content, embedded media, or structured data.
Tables and structured information that require accurate parsing.
Higher success rates.
If precision and depth are priorities for your application, extract_depth = "advanced" is the recommended choice.

Best Practices for Crawl
Learn how to effectively use Tavily‚Äôs Crawl API to extract and process web content.

‚Äã
When to Use crawl vs map
‚Äã
Use Crawl when you need:
Full content extraction from pages
Deep content analysis
Processing of paginated or nested content
Extraction of specific content patterns
Integration with RAG systems
‚Äã
Use Map when you need:
Quick site structure discovery
URL collection without content extraction
Sitemap generation
Path pattern matching
Domain structure analysis
‚Äã
Use Cases
‚Äã
1. Deep or Unlinked Content
Many sites have content that‚Äôs difficult to access through standard means:
Deeply nested pages not in main navigation
Paginated archives (old blog posts, changelogs)
Internal search-only content
Best Practice:

Copy

Ask AI
{
  "url": "example.com",
  "max_depth": 3,
  "max_breadth": 50,
  "limit": 200,
  "select_paths": ["/blog/.*", "/changelog/.*"],
  "exclude_paths": ["/private/.*", "/admin/.*"]
}
‚Äã
2. Structured but Nonstandard Layouts
For content that‚Äôs structured but not marked up in schema.org:
Documentation
Changelogs
FAQs
Best Practice:

Copy

Ask AI
{
  "url": "docs.example.com",
  "max_depth": 2,
  "extract_depth": "advanced",
  "select_paths": ["/docs/.*"]
}
‚Äã
3. Multi-modal Information Needs
When you need to combine information from multiple sections:
Cross-referencing content
Finding related information
Building comprehensive knowledge bases
Best Practice:

Copy

Ask AI
{
  "url": "example.com",
  "max_depth": 2,
  "instructions": "Find all documentation pages that link to API reference docs",
  "extract_depth": "advanced"
}
‚Äã
4. Rapidly Changing Content
For content that updates frequently:
API documentation
Product announcements
News sections
Best Practice:

Copy

Ask AI
{
  "url": "api.example.com",
  "max_depth": 1,
  "max_breadth": 100,
  "extract_depth": "basic"
}
‚Äã
5. Behind Auth / Paywalls
For content requiring authentication:
Internal knowledge bases
Customer help centers
Gated documentation
Best Practice:

Copy

Ask AI
{
  "url": "help.example.com",
  "max_depth": 2,
  "select_domains": ["^help\\.example\\.com$"],
  "exclude_domains": ["^public\\.example\\.com$"]
}
‚Äã
6. Complete Coverage / Auditing
For comprehensive content analysis:
Legal compliance checks
Security audits
Policy verification
Best Practice:

Copy

Ask AI
{
  "url": "example.com",
  "max_depth": 3,
  "max_breadth": 100,
  "limit": 1000,
  "extract_depth": "advanced",
  "instructions": "Find all mentions of GDPR and data protection policies"
}
‚Äã
7. Semantic Search or RAG Integration
For feeding content into LLMs or search systems:
RAG systems
Enterprise search
Knowledge bases
Best Practice:

Copy

Ask AI
{
  "url": "docs.example.com",
  "max_depth": 2,
  "extract_depth": "advanced",
  "include_images": true
}
‚Äã
8. Known URL Patterns
When you have specific paths to crawl:
Sitemap-based crawling
Section-specific extraction
Pattern-based content collection
Best Practice:

Copy

Ask AI
{
  "url": "example.com",
  "max_depth": 1,
  "select_paths": ["/docs/.*", "/api/.*", "/guides/.*"],
  "exclude_paths": ["/private/.*", "/admin/.*"]
}
‚Äã
Performance Considerations
‚Äã
Depth vs. Performance
Each level of depth increases crawl time exponentially
Start with max_depth: 1 and increase as needed
Use max_breadth to control horizontal expansion
Set appropriate limit to prevent excessive crawling
‚Äã
Resource Optimization
Use basic extract_depth for simple content
Use advanced extract_depth only when needed
Set appropriate max_breadth based on site structure
Use select_paths and exclude_paths to focus crawling
‚Äã
Rate Limiting
Respect site‚Äôs robots.txt
Implement appropriate delays between requests
Monitor API usage and limits
Use appropriate error handling for rate limits
‚Äã
Best Practices Summary
Start Small
Begin with limited depth and breadth
Gradually increase based on needs
Monitor performance and adjust
Be Specific
Use path patterns to focus crawling
Exclude irrelevant sections
Optimize Resources
Choose appropriate extract_depth
Set reasonable limits
Use include_images only when needed
Handle Errors
Implement retry logic
Monitor failed results
Handle rate limits appropriately
Security
Respect robots.txt
Use appropriate authentication
Exclude sensitive paths
Integration
Plan for data processing
Consider storage requirements
Design for scalability
‚Äã
Common Pitfalls
Excessive Depth
Avoid setting max_depth too high
Start with 1-2 levels
Increase only if necessary
Unfocused Crawling
Use instructions for guidance
Resource Overuse
Monitor API usage
Set appropriate limits
Use basic extract_depth when possible
Missing Content
Verify path patterns
Monitor crawl coverage
‚Äã
Integration with Map
Consider using Map before Crawl to:
Discover site structure
Identify relevant paths
Plan crawl strategy
Validate URL patterns
Example workflow:
Use Map to get site structure
Analyze paths and patterns
Configure Crawl with discovered paths
Execute focused crawl
‚Äã
Conclusion
Tavily‚Äôs Crawl API is powerful for extracting structured content from websites. By following these best practices, you can:
Optimize crawl performance
Ensure complete coverage
Maintain resource efficiency
Build robust content extraction pipelines
Remember to:
Start with limited scope
Use appropriate parameters
Monitor performance
Handle errors gracefully
Respect site policies